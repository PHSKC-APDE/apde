---
title: "etl_qa_run_pipeline()"
format: 
  gfm:
    prefer-html: false
    self-contained: true
  pdf: default
  html: default
editor: visual
---

```{r echo=FALSE}
#| warning: false
#| message: false
library(apde)
library(kableExtra)

# function to beautify tables
pretty_kable <- function(dt) { 
  knitr::kable(dt, format = 'markdown')
}
```

# Introduction

The `etl_qa_run_pipeline()` function is designed to identify data quality issues before performing data analysis. To maximize its effectiveness, it is recommended to use this tool throughout the ETL pipeline. It can be run after extraction to ensure that the data provided by the sources is as expected and has not been corrupted during transmission. After transformation, it can check for potential coding errors. Finally, after loading the data to its near-final destination, it can be run again—perhaps by an analyst familiar with the data—to check for any remaining data quality issues.

In essence, the function draws data from SQL Server, a `data.table` or `data.frame` in R's memory, or via the [`rads::get_data_xxx`](https://github.com/PHSKC-APDE/rads/wiki/get_data) functions. It identifies missing data and assesses changes in numeric statistics and categorical frequency distributions over time. Results are provided both in a table that can be filtered and reviewed (and easily read into Tableau) and as PDF graphs that help identify changes over time. Optionally, it can check if variables align with CHI (Community Health Indicators) standards.

# Example birth data QA output

The function returns a named list containing:

-   `config`: Configuration settings used for the analysis
-   `initial`: Initial ETL QA results
-   `final`: Final ETL QA data.table summaries that are exported and used in plots
-   `exported`: File paths for exported tables and plots

Most of the time, you will use the exported data, which comprises two PDFs and one Excel workbook. Here is a peek at the output generated by the example code that will be discussed in detail below.

## PDF 1: Missingness

The graph below shows the percentage of observations (i.e., rows) that are missing at each time period.<br><br> ![](etl_qa_missing.jpg){#fig-missing}

## PDF 2a: Numeric statistics

The graph below shows basic statistics for numeric variables at each time period.<br><br> ![](etl_qa_continuous.jpg){#fig-continuous}

## PDF 2b: Date statistics

The graph below shows basic statistics for date variables at each time period. Note that datetime variables such as `POSIXct` and `POSIXt` in R and `datetime`, `datetime2`, and `smalldatetime` in SQL will assessed as dates.<br><br> ![](etl_qa_date.jpg){#fig-date}

## PDF 2c: Categorical frequencies

The graph below shows the proportion of observations with a given value at each time period. It displays the top eight most frequent values PLUS missing values (dotted line) PLUS all other values combined and labeled 'Other values'. Note that numeric and date variables with six or fewer distinct values will be treated as a categorical.<br><br> ![](etl_qa_categorical.jpg){#fig-categorical}

As you can see in the figure from PDF 2c, the proportion lines can overlap and it can be difficult to identify all the differences that might be of interest. For a more detailed exploration of this data, you can view the tables stored in the exported Excel file (which are also saved by the function in the `final` item of the returned list). Here is a snapshot of each of tables:

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Perform the calculations, but don't directly display output
myconnection <- rads::validate_hhsaw_key()
qaSQL <- etl_qa_run_pipeline(
  data_source_type = 'sql_server',
  connection = myconnection,
  data_params = list(
    schema_table = 'birth.final_analytic',
    time_var = 'chi_year',
    time_range = c(2013, 2022),
    cols =c('chi_age', 'race4', 'birth_weight_grams', 'birthplace_city', 
            'num_prev_cesarean', 'mother_date_of_birth'), 
    check_chi = TRUE
  ), 
  output_directory = 'C:/temp/', 
  digits_mean = 3
)
```

Here is a sample of the 'missingness' worksheet saved to the Excel file. You would be able to access this in R by typing something like `results$final$missingness`. As per the default settings (which can be adjusted), it is flagging \>= 3% *absolute* changes in the proportion missing between adjacent time periods.

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Get a sample of the final missingness table
sample_missing <- qaSQL$final$missingness
keepers <- which(!is.na(sample_missing$abs_change))[1]
keepers <- (keepers - 4):(keepers + 4)
pretty_kable(sample_missing[keepers])

```

Similarly, here is a sample from the 'values' worksheet (`results$final$values`). Here the default settings flag rows with a \>= 2% *relative* change in the mean value between adjacent years.

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Get a sample of the final values table
sample_values <- qaSQL$final$values
keepers <- which(!is.na(sample_values$rel_mean_change))[1]
keepers <- (keepers - 4):(keepers + 4)
pretty_kable(sample_values[keepers])
```

Finally, if you opt for checking the CHI values, your Excel workbook will have a sheet called 'chi_standards', which can be accessed in the returned object (`myresult$final$chi_standards`).

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Get a sample of the final values table
sample_chi <- qaSQL$final$chi_standards
keepers <- which(!is.na(sample_chi$problem))[1]
keepers <- (keepers - 4):(keepers + 4)
pretty_kable(sample_chi[keepers])
```

# Setting up the environment

All you have to is load the package:

```{r echo=TRUE, results='hide'}
#| warning: false
#| message: false
library(apde)
```

# Function arguments

Since this function allows the user to QA data in three different data sources that need different specifications, understanding the arguments is not as simple as typing `args(etl_qa_run_pipeline)`. Here are all the possible arguments as described in the helpfile (`?etl_qa_run_pipeline`):

## `data_source_type`

Character string specifying the type of data source. Must be one of `'r_dataframe'`, `'sql_server'`, or `'rads'`. \## `connection`\
A DBIConnection object.<br>**Required only when `data_source_type = 'sql_server'`**.

## `data_params`

List of data related parameters specific to the data source. Not all parameters are needed for all data sources. Please review the examples for details.

-   

    ### `data_params$check_chi`

    Logical vector of length 1. When `check_chi = TRUE`, function will add any available CHI related variables to `data_params$cols` and will assess whether their values align with standards in `rads.data::misc_chi_byvars`.<br>Default is `check_chi = FALSE`.

-   

    ### `data_params$cols`

    Character vector specifying the column names to analyze, e.g., `cols = c('race4', 'birth_weight_grams', 'birthplace_city')`.

-   

    ### `data_params$time_range`

    Character vector of length 2 specifying the start and end of the time range, e.g., `time_range = c(2015, 2024)`.

-   

    ### `data_params$time_var`

    Character string specifying the time interval variable, e.g., `time_var = 'chi_year'`.

-   

    ### `data_params$data`

    Name of a data.frame or data.table that you want to assess with this function, e.g., `data = myDataTable`.<br>**Required only when `data_source_type = 'r_dataframe'`**.

-   

    ### `data_params$function_name`

    Character string specifying the relevant rads::get_data_xxx function, e.g., `function_name = 'get_data_birth'`.<br>**Required only when `data_source_type = 'rads'`**.

-   

    ### `data_params$kingco`

    Logical vector of length 1. Identifies whether you want limit the data to King County.<br>**Required only when `data_source_type = 'rads'`**.<br>Default is `kingco = TRUE`.

-   

    ### `data_params$version`

    Character string specifying either `'final'` or `'stage'`.<br>**Required only when `data_source_type = 'rads'`**.<br>Default is `version = 'stage'`.

-   

    ### `data_params$schema_table`

    The name of the schema and table to be accessed within the SQL Server connection. Must be in the form myschema.mytable, with a period as a separator.<br>**Required only when `data_source_type = 'sql_server'`**.

## `output_directory`

Character string specifying the directory where output files will be saved. If `NULL`, the current working directory is used.<br>Default is `output_directory = NULL`.

## `digits_mean`

Integer specifying the number of decimal places for rounding the reported mean, median, min, and max.<br>Default is `digits_mean = 0`.

## `digits_prop`

Integer specifying the number of decimal places for rounding proportions.<br>Default is `digits_prop = 3`.

## `abs_threshold`

Numeric threshold for flagging absolute percentage changes in proportions. Permissible range is \[0, 100\].<br>Default is `abs_threshold = 3`.

## `rel_threshold`

Numeric threshold for flagging relative percentage changes in means and medians. Permissible range is \[0, 100\].<br>Default is `rel_threshold = 2`.

# Conclusion

Remember to consult the individual function documentation for more detailed information on usage and parameters. Happy coding!

-- *`r paste0("Updated by ", Sys.getenv("USERNAME"), ", ", Sys.Date())`*
